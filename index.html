<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>
  Cookbook Documentation
  
    &mdash; Documentation by YARD 0.8.7.6
  
</title>

  <link rel="stylesheet" href="css/style.css" type="text/css" charset="utf-8" />

  <link rel="stylesheet" href="css/common.css" type="text/css" charset="utf-8" />

<script type="text/javascript" charset="utf-8">
  hasFrames = window.top.frames.main ? true : false;
  relpath = '';
  framesUrl = "frames.html#!file.README.html";
</script>


  <script type="text/javascript" charset="utf-8" src="js/jquery.js"></script>

  <script type="text/javascript" charset="utf-8" src="js/app.js"></script>


  </head>
  <body>
    <div id="header">
      <div id="menu">
  
    <a href="index.html">Index</a> &raquo; 
    <span class="title">File: README</span>
  

  <div class="noframes"><span class="title">(</span><a href="." target="_top">no frames</a><span class="title">)</span></div>
</div>

      <div id="search">
  
    <a class="full_list_link" id="cookbooks_list_link"
        href="cookbooks_list.html">
      Cookbook List
    </a>
  
    <a class="full_list_link" id="recipes_list_link"
        href="recipes_list.html">
      Recipe List
    </a>
  
    <a class="full_list_link" id="resources_list_link"
        href="resources_list.html">
      Resource List
    </a>
  
    <a class="full_list_link" id="definitions_list_link"
        href="definitions_list.html">
      Definitions List
    </a>
  
    <a class="full_list_link" id="class_list_link"
        href="class_list.html">
      Libraries List
    </a>
  
</div>
      <div class="clear"></div>
    </div>

    <iframe id="search_frame"></iframe>

    <div id="content">

<div id="filecontents">
  <h1>Chef Cookbooks Documentation</h1>
  <div id='filecontents'><h1>apache_spark</h1>

<p><a href="https://travis-ci.org/clearstorydata-cookbooks/apache_spark"><img src="https://travis-ci.org/clearstorydata-cookbooks/apache_spark.svg?branch=master" alt="Build Status"></a></p>

<p>This cookbook installs and configures Apache Spark.</p>

<ul>
<li>GitHub: <a href="https://github.com/clearstorydata-cookbooks/apache_spark">https://github.com/clearstorydata-cookbooks/apache_spark</a></li>
<li>Chef Supermarket: <a href="https://supermarket.chef.io/cookbooks/apache_spark">https://supermarket.chef.io/cookbooks/apache_spark</a></li>
<li>Travis CI: <a href="https://travis-ci.org/clearstorydata-cookbooks/apache_spark">https://travis-ci.org/clearstorydata-cookbooks/apache_spark</a></li>
<li>Documentation: <a href="http://clearstorydata-cookbooks.github.io/apache_spark/chef/apache_spark.html">http://clearstorydata-cookbooks.github.io/apache_spark/chef/apache_spark.html</a></li>
</ul>

<h2>Overview</h2>

<p>This cookbook installs and configures Apache Spark. Currently, only the standalone deployment mode
is supported. Future work:</p>

<ul>
<li>YARN and Mesos deployment modes</li>
<li>Support installing from Cloudera and HDP Spark packages.</li>
</ul>

<h2>Compatibility</h2>

<p>The following platforms are currently tested:</p>

<ul>
<li>Ubuntu 12.04</li>
<li>CentOS 6.5</li>
</ul>

<p>The following platforms are not tested but will probably work (tests coming soon):</p>

<ul>
<li>Fedora 21</li>
<li>Ubuntu 14.04</li>
</ul>

<h2>Configuration</h2>

<ul>
<li><code>node[&#39;apache_spark&#39;][&#39;install_mode&#39;]</code>: <code>tarball</code> to install from a downloaded tarball,
or <code>package</code> to install from an OS-specific package.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;download_url&#39;]</code>: the URL to download Apache Spark binary distribution
tarball in the <code>tarball</code> installation mode.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;checksum&#39;]</code>: SHA256 checksum for the Apache Spark binary distribution
tarball.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;pkg_name&#39;]</code>: package name to install in the <code>package</code> installation mode.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;pkg_version&#39;]</code>: package version to install in the <code>package</code> installation
mode.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;install_dir&#39;]</code>: target directory to install Spark to in the <code>tarball</code>
installation mode. In the <code>package</code> mode, this must be set to the directory that the package
installs Spark into.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;install_base_dir&#39;]</code>: in the <code>tarball</code> installation mode, this is where
the tarball is actually extracted, and a symlink pointing to the subdirectory containing a
specific Spark version is created at <code>node[&#39;apache_spark&#39;][&#39;install_dir&#39;]</code>.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;user&#39;]</code>: UNIX user to create for running Spark.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;group&#39;]</code>: UNIX group to create for running Spark.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;master_host&#39;]</code>: Spark standalone-mode workers will connect to
this host.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;master_bind_ip&#39;]</code>: the IP the master should bind to. This
should be set in such a way that workers will be able to connect to the master.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;master_port&#39;]</code>: the port for the Spark standalone master to
listen on.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;master_webui_port&#39;]</code>: Spark standalone master web UI port.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_bind_ip&#39;]</code>: the IP address workers bind to.
They bind to all network interfaces by default.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_webui_port&#39;]</code>: the port for the Spark worker web UI
to listen on.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;job_dir_days_retained&#39;]</code>: <code>app-...</code> subdirectories of
<code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_work_dir&#39;]</code> older than this number of days will be
deleted periodically on worker nodes to prevent unbounded accumulation. These directories contain
Spark executor stdout/stderr logs. The directories will still be retained to honor
<code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;job_dir_num_retained&#39;]</code>.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;job_dir_num_retained&#39;]</code>: the minimum number of Spark
executor log directories (<code>app-...</code>) to retain, regardless of creation time.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_dir_cleanup_log&#39;]</code>: log file path for the Spark
executor log directories cleanup script.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_cores&#39;]</code>: the number of &quot;cores&quot; (threads) to allocate
on each worker node.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_work_dir&#39;]</code>: the directory to store Spark
executor logs and Spark job jars.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_memory_mb&#39;]</code>: the amount of memory in MB to allocate
to each worker (i.e. the maximum total memory used by different applications&#39; executors running
on a worker node).</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;default_executor_mem_mb&#39;]</code>: the default amount of memory
to be allocated to a Spark application&#39;s executor on each node.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;log_dir&#39;]</code>: the log directory for Spark masters and workers.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;daemon_root_logger&#39;]</code>: the <code>spark.root.logger</code> property
is set to this.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;max_num_open_files&#39;]</code>: the maximum number of open files to
set using <code>ulimit</code> before launching a worker.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;java_debug_enabled&#39;]</code>: whether Java debugging options are
to be enabled for Spark processes. Note: currently, this option is not working as intended.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;default_debug_port&#39;]</code>: default Java debug port to use.
A free port is chosen if this port is unavailable.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;master_debug_port&#39;]</code>: default Java debug port to use for
Spark masters. A free port is chosen if this port is unavailable.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_debug_port&#39;]</code>: default Java debug port to use for
Spark workers. A free port is chosen if this port is unavailable.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;executor_debug_port&#39;]</code>: default Java debug port to use for
Spark standalone executors. A free port is chosen if this port is unavailable.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;common_extra_classpath_items&#39;]</code>: common classpath items to
add to Spark application driver and executors (but not Spark master and worker processes).</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_dir&#39;]</code>: Set to a non-nil value to tell the spark worker to use an alternate directory for spark scratch space</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;worker_opts&#39;]</code>: Set to a non-nil value to pass along any additional settings to the spark worker. E.G.: <code>-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.appDataTtl=86400</code>.  Ideal for worker options only that you do not want in the default configuration file.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;conf&#39;][&#39;...&#39;]</code>: Spark configuration options that go into the default
Spark configuration file. See <a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a> for details.</li>
<li><code>node[&#39;apache_spark&#39;][&#39;standalone&#39;][&#39;local_dirs&#39;]</code>: a list of local directories to use on workers.
This is where map output files are stored, so these directories should have enough space
available.</li>
</ul>

<h2>Testing</h2>

<h3>ChefSpec</h3>

<pre class="code ruby"><code class="ruby"><span class='id identifier rubyid_bundle'>bundle</span> <span class='id identifier rubyid_install'>install</span>
<span class='id identifier rubyid_bundle'>bundle</span> <span class='id identifier rubyid_exec'>exec</span> <span class='id identifier rubyid_rspec'>rspec</span>
</code></pre>

<h3>Test Kitchen</h3>

<pre class="code ruby"><code class="ruby"><span class='id identifier rubyid_bundle'>bundle</span> <span class='id identifier rubyid_install'>install</span>
<span class='id identifier rubyid_bundle'>bundle</span> <span class='id identifier rubyid_exec'>exec</span> <span class='id identifier rubyid_kitchen'>kitchen</span> <span class='id identifier rubyid_test'>test</span>
</code></pre>

<h2>Contributing</h2>

<p>If you would like to contribute this cookbook&#39;s development, please follow the steps below:</p>

<ul>
<li>Fork this repository on GitHub</li>
<li>Make your changes</li>
<li>Run tests</li>
<li>Submit a pull request</li>
</ul>

<h2>License</h2>

<p>Apache License 2.0</p>

<p><a href="https://www.apache.org/licenses/LICENSE-2.0">https://www.apache.org/licenses/LICENSE-2.0</a></p>
</div>

<h1>Cookbooks List</h1>
<table>
  <thead>
    <tr>
      <th>Cookbook</th>
      <th>Description</th>
      <th>Version</th>
    </tr>
  </thead>
  <tbody>
  
  
  
    <tr class="r1">
      <td><span class='object_link'><a href="chef/apache_spark.html" title="chef::apache_spark (cookbook)">apache_spark</a></span></td>
      <td>A cookbook to install and configure Apache Spark</td>
      <td>1.2.3</td>
    </tr>
    
  
  </tbody>
</table>

</div>
</div>

    <div id="footer">
  Generated on Wed Aug 19 22:29:32 2015 by
  <a href="http://yardoc.org" title="Yay! A Ruby Documentation Tool" target="_parent">yard</a>
  0.8.7.6 (ruby-2.1.5).
</div>

  </body>
</html>
